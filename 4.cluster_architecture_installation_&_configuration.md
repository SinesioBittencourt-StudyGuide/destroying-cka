
# Provision underlying infrastructure to deploy a Kubernetes cluster 🛠

Every Kubernetes Cluster solution has it's own specification of the minimum recommended infrastructure for deploying a cluster, for the sake of simplicity, **kubeadm** minimal requirements are considered.

## What hardware infrastructure is recommended to deploy a Kubernetes Cluster using kubeadm?

- Linux Host
- Network Connectivity between all machines in the cluster
- Unique **hostname**, MAC address and product_uuid for every node
- Required **Ports** opened:

|Protocol|Direction|Port Range|Purpose|Used by|
|-|-|-|-|-|
|TCP|Inbound|6443*|Kubernetes API server|All|
|TCP|Inbound|2379-2380|etcd server client API|kube-apiserver, etcd|
|TCP|Inbound|10250|kubelet API|Self, Control plane|
|TCP|Inbound|10251|kube-scheduler|Self|
|TCP|Inbound|10252|kube-controller-manager|Self|

- Swap disabled
- Recommended Hardware setup:

| Role     |        Minimal Recommended Memory              |  Minimal Recommended CPU(cores) |
| ----------  |:------------------------:| -----:|
| control     | 2 GB | 2     |
| worker      | 2 GB | 2     |

This infrastructure recommended specifications are based on a simple cluster deployment, adding any other specific solutions may change the required hardware for running a stable cluster application.

### Steps to be performed on each machine: 
1. Setup minimum recommended hardware configuration(when working with VMs or look-alikes)
2. Set up ip addresses for each node
3. Set up specific hostname for the nodes
4. Check network adapters
5. Configure iptables
6. Install Docker
7. Install kubeadm
8. Install kubelet
9. Install kubectl

Detailed steps and used commands on [Install kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/).
&nbsp;

&nbsp;
# Use Kubeadm to install a basic cluster ⚙️

Kubeadm can be used to deploy a minimum viable K8s cluster.
## Steps
- Install a single control-plane Kubernetes cluster
- Run **kubeadm init** on the control node
```bash
# the --control-plane-endpoint is used for future cluster HA upgrade
sudo kubeadm init --control-plane-endpoint=cluster-endpoint
```

> The init command output directs the steps of cluster configuration, save the "join" commands for setting up worker nodes.

- Setup k8s config file and user permissions

```shell
mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
- Install a Pod network on the cluster so that your Pods can talk to each other
```bash
# weave net add-on 
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

```
- Join worker nodes
```bash
kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>
```

-  Check if the workers nodes where created
```shell
kubectl get nodes
```

&nbsp;

&nbsp;
# Manage role based access control (RBAC)

O Usuário precisa de um certificado durante o processo de autenticação para acessar o Kubernetes

Auth Modules -> usado para inserção de certificações de clients
Tipos de Clients
- Users -> client certificates
- Services -> tokens

**A User is just a set of client certificates**

Se não autenticar recebe um -> rejected with HTTP status code 401

/etc/kubernetes/manifests/kube-apiserver.yaml -> current setting

Authorization mode implements policies

Autenticação no Kubernetes significa entregar os certificados corretos para acessar o Cluster Kubernetes.


RBAC API Objects
- Role -> configura permissões no namespace específico, criou role, especifica o namespace
- ClusterRole -> Configura permissões sem a restrição de estar atrelado a um namespace específico


Role and ClusterRole
- Set of permissions
- Additive, no deny rules

If you want to define a role within a namespace, use a Role; if you want to define a role cluster-wide, use a ClusterRole.

- RoleBinding -> grants the permissions defined in a Role to a user. Holds a list of subjects and a reference to the Role, grants permissions within a specific namespace
- ClusterRoleBinding ->  grants permissions access cluster-wide

RoleBinding may reference a Role in the same namespace
RoleBinding can reference a ClusterRole
If you want to bind a ClusterRole to all the namespaces in your cluster, you use a ClusterRoleBinding

After creating a binding you can not change the Role or ClusterRole that it refers to.
kubectl auth reconcile

To delimit the resource and subresource an RBAC rule is created for
you need to specify on resources this: 
  resources: ["pods", "pods/log"]
just like the API req in HTTP, with the path

A specific resource(secret, configmap etc) also can be used like this to define the created resource to be within the Role

resourceNames: ["my-configmap"]


In the YAML File...

Role and ClusterRole binds a Role beying created to rules, it defines the rules

RoleBinding and ClusterRoleBinding binds a role to subjects: groups, users and serviceAccount
&nbsp;

&nbsp;
# Implement etcd backup and restore

ETCDCTL_API=3 etcdctl snapshot save -h

sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt

ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb
&nbsp;

&nbsp;
# Perform a version upgrade on a Kubernetes cluster using Kubeadm
- yum makecache fast 
- yum list --showduplicates kubeadm --disableexcludes=kubernetes
- yum install -y kubeadm-<version-number> --disableexcludes=kubernetes
- kubectl drain control --ignore-daemonsets
- sudo kubeadm upgrade plan
- sudo kubeadm upgrade apply v<whatever>
- kubectl uncordon control
- sudo yum install -y kubelet-<latestversion> kubectl-<latestversion>
- sudo systemctl daemon-reload
- sudo systemctl restart kubelet

On worker nodes, each by each
- kubectl drain <nodename> --ignore-daemonsets --force --delete-local-data
    if metrics server complain delete it, kubectl delete deployment metrics-server -n kube-system
- worker sudo kubeadm upgrade node
- worker sudo yum install -y kubelet-<latestversion> kubectl-<latestversion> --disableexcludes=kubernetes
- worker sudo system daemon-reload; sudo systemctl restart kubelet
- control kubectl undordon <nodename>
- control kubectl get nodes
&nbsp;

&nbsp;
# Manage a highly#available Kubernetes cluster

Esse aqui fica para depois
