# namespaces
- provides a scope for names
- cannot be nested inside one another
- each k8s resource can only be in one namespace
- divide resources between multiple users
- labels used to distinguish resources within the same namespace

# Provision underlying infrastructure to deploy a Kubernetes cluster



# Manage role based access control (RBAC)

O Usuário precisa de um certificado durante o processo de autenticação para acessar o Kubernetes

Auth Modules -> usado para inserção de certificações de clients
Tipos de Clients
- Users -> client certificates
- Services -> tokens

**A User is just a set of client certificates**

Se não autenticar recebe um -> rejected with HTTP status code 401

/etc/kubernetes/manifests/kube-apiserver.yaml -> current setting

Authorization mode implements policies

Autenticação no Kubernetes significa entregar os certificados corretos para acessar o Cluster Kubernetes.


RBAC API Objects
- Role -> configura permissões no namespace específico, criou role, especifica o namespace
- ClusterRole -> Configura permissões sem a restrição de estar atrelado a um namespace específico


Role and ClusterRole
- Set of permissions
- Additive, no deny rules

If you want to define a role within a namespace, use a Role; if you want to define a role cluster-wide, use a ClusterRole.

- RoleBinding -> grants the permissions defined in a Role to a user. Holds a list of subjects and a reference to the Role, grants permissions within a specific namespace
- ClusterRoleBinding ->  grants permissions access cluster-wide

RoleBinding may reference a Role in the same namespace
RoleBinding can reference a ClusterRole
If you want to bind a ClusterRole to all the namespaces in your cluster, you use a CliusterRoleBinding

After creating a binding you can not change the Role or ClusterRole that it refers to.
kubectl auth reconcile

To delimit the resource and subresource an RBAC rule is created for
you need to specify on resources this: 
  resources: ["pods", "pods/log"]
just like the API req in HTTP, with the path

A specific resource(secret, configmap etc) also can be used like this to define the created resource to be within the Role

resourceNames: ["my-configmap"]


In the YAML File...

Role and ClusterRole binds a Role beying created to rules, it defines the rules

RoleBinding and ClusterRoleBinding binds a role to subjects: groups, users and serviceAccount

# Use Kubeadm to install a basic cluster

- kubeadm init
- O próprio init retorna os comandos que são necessários para configurar o cluster

- kubeadm token

Para fazer funcionar para usuários comuns(non-root)
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config



# Manage a highly#available Kubernetes cluster

Esse aqui fica para depois

# Perform a version upgrade on a Kubernetes cluster using Kubeadm
- yum makecache fast 
- yum list --showduplicates kubeadm --disableexcludes=kubernetes
- yum install -y kubeadm-<version-number> --disableexcludes=kubernetes
- kubectl drain control --ignore-daemonsets
- sudo kubeadm upgrade plan
- sudo kubeadm upgrade apply v<whatever>
- kubectl uncordon control
- sudo yum install -y kubelet-<latestversion> kubectl-<latestversion>
- sudo systemctl daemon-reload
- sudo systemctl restart kubelet

On worker nodes, each by each
- kubectl drain <nodename> --ignore-daemonsets --force --delete-local-data
    if metrics server complain delete it, kubectl delete deployment metrics-server -n kube-system
- worker sudo kubeadm upgrade node
- worker sudo yum install -y kubelet-<latestversion> kubectl-<latestversion> --disableexcludes=kubernetes
- worker sudo system daemon-reload; sudo systemctl restart kubelet
- control kubectl undordon <nodename>
- control kubectl get nodes


# Implement etcd backup and restore

ETCDCTL_API=3 etcdctl snapshot save -h

sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt

ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb
