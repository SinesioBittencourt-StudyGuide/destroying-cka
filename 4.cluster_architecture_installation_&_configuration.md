
# Provision underlying infrastructure to deploy a Kubernetes cluster ðŸ› 

Every Kubernetes Cluster solution has it's own specification of the minimum recommended infrastructure for deploying a cluster, for the sake of simplicity, **kubeadm** minimal requirements are considered.

## What hardware infrastructure is recommended to deploy a Kubernetes Cluster using kubeadm?

- Linux Host
- Network Connectivity between all machines in the cluster
- Unique **hostname**, MAC address and product_uuid for every node
- Required **Ports** opened:

|Protocol|Direction|Port Range|Purpose|Used by|
|-|-|-|-|-|
|TCP|Inbound|6443*|Kubernetes API server|All|
|TCP|Inbound|2379-2380|etcd server client API|kube-apiserver, etcd|
|TCP|Inbound|10250|kubelet API|Self, Control plane|
|TCP|Inbound|10251|kube-scheduler|Self|
|TCP|Inbound|10252|kube-controller-manager|Self|

- Swap disabled
- Recommended Hardware setup:

| Role     |        Minimal Recommended Memory              |  Minimal Recommended CPU(cores) |
| ----------  |:------------------------:| -----:|
| control     | 2 GB | 2     |
| worker      | 2 GB | 2     |

This infrastructure recommended specifications are based on a simple cluster deployment, adding any other specific solutions may change the required hardware for running a stable cluster application.

### Steps to be performed on each machine:

1. Setup minimum recommended hardware configuration(when working with VMs or look-alikes)
2. Set up ip addresses for each node
3. Set up specific hostname for the nodes
4. Check network adapters
5. Configure iptables
6. Install Docker
7. Install kubeadm
8. Install kubelet
9. Install kubectl

Detailed steps and used commands on [Install kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/).
&nbsp;

&nbsp;

# Use Kubeadm to install a basic cluster âš™ï¸

Kubeadm can be used to deploy a minimum viable K8s cluster.

## Steps

- Install a single control-plane Kubernetes cluster
- Run **kubeadm init** on the control node

```bash
# the --control-plane-endpoint is used for future cluster HA upgrade
sudo kubeadm init --control-plane-endpoint=cluster-endpoint
```

> The init command output directs the steps of cluster configuration, save the "join" commands for setting up worker nodes.

- Setup k8s config file and user permissions

```shell
mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

- Install a Pod network on the cluster so that your Pods can talk to each other

```bash
# weave net add-on 
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

```

- Join worker nodes

```bash
kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>
```

- Check if the workers nodes where created

```shell
kubectl get nodes
```

Reference: [Creating a cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).
&nbsp;

&nbsp;

# Manage role based access control (RBAC)

## RBAC

Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.

The user needs a certificate during the process of authentication to access the Kubernetes.

> **A User is just a set of client certificates**

## RBAC API Objects â¨™

- Permissions are additive, there are no deny rules.

> **If not specifyied in the Object, you cannot do.**

- A Kubernetes object always has to be either namespaced or not.

### Role âœ¨

- Configures Permissions in a specific namespace(*whenever you create a role, specify the namespace*).

Example:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

### ClusterRole âœ¨

- Configures a set os permissions without being attached to a specific namespace.

Because they are cluster-scoped, you can also use them to grant access to:

- cluster-scoped resources(like nodes)
- non-resources endpoints(like /healthz)
- namespaced resources across all namespaces (like Pods)

Example:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: secret-reader
rules:
- apiGroups: [""]
  #
  # at the HTTP level, the name of the resource for accessing Secret
  # objects is "secrets"
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```


### RoleBinding âœ¨

- Grants the permissions defined in a Role to a user. Holds a list of subjects and a reference to the Role.
- RoleBinding may reference a Role in the same namespace.
- RoleBinding can reference a ClusterRole

Example:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows "jane" to read pods in the "default" namespace.
# You need to already have a Role named "pod-reader" in that namespace.
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# You can specify more than one "subject"
- kind: User
  name: jane # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # "roleRef" specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
```

- RoleBinding can also reference ClusterRole, you can define a set of common roles across your cluster, then reuse them in multiple namespaces. &nbsp; The RoleBinding works like a filter, it turns the cluster-wide permission into a specific one for a user within a namespace.

Example:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows "dave" to read secrets in the "development" namespace.
# You need to already have a ClusterRole named "secret-reader".
kind: RoleBinding
metadata:
  name: read-secrets
  #
  # The namespace of the RoleBinding determines where the permissions are granted.
  # This only grants permissions within the "development" namespace.
  namespace: development
subjects:
- kind: User
  name: dave # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

### ClusterRoleBinding âœ¨

- Grants permissions access cluster-wide.
- It can grant permissions to any user in a specific namespace to perform actions in any namespace on the cluster.

Example:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
# This cluster role binding allows anyone in the "manager" group to read secrets in any namespace.
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

> After creating a binding you can not change the Role or ClusterRole that it refers to.

```bash
kubectl auth reconcile # creates or update a manifest file containing RBAC objects, and handles deleting and recreating binding objects if required to change the role they refer to.
```

## Referring to Resources

> RBAC refers to resources using exactly the same name that appears in the URL for the relevant API endpoint.

- RBAC can refer to resources and subresources.

```http
GET /api/v1/namespaces/{namespace}/pods/{name}/log
```

 To represent this in an RBAC role, use a slash ( / ) to delimit the resource and subresource. Example: 

 ```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-and-pod-logs-reader
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"] # this is how you refer to a subresource
  verbs: ["get", "list"]
 ```

Requests can be restricted to individual instances of a resource using **resourceNames** list. Example: 

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: configmap-updater
rules:
- apiGroups: [""]
  # restricts its subject to only get or update a ConfigMap named my-configmap
  # at the HTTP level, the name of the resource for accessing ConfigMap objects is "configmaps"
  resources: ["configmaps"]
  resourceNames: ["my-configmap"]
  verbs: ["update", "get"]
```

## RBAC Verbs List

- get, list (read-only)

- create, update, patch, delete, deletecollection (read-write)

## Wrap-up ðŸ”–

### When to use it?

- If you want to define a role within a namespace, use a Role
- If you want to define a role cluster-wide, use a ClusterRole.
- If you want to attach a Role or ClusterRole to a specific user, use RoleBinding
- If you want to bind a ClusterRole to all the namespaces in your cluster, you use a ClusterRoleBinding.

### In the YAML File

- Role and ClusterRole creates and bind a role to rules defined on the file.

- RoleBinding and ClusterRoleBinding binds a role to subjects: groups, users and serviceAccount.

### Specs

API Resource | API Version | Namespaced | Kind
---|---|---|---|
roles | rbac.authorization.k8s.io/v1 | true | Role
rolebindings | rbac.authorization.k8s.io/v1 | true | RoleBinding
clusterroles | rbac.authorization.k8s.io/v1 | false | ClusterRole
clusterrolebindings | rbac.authorization.k8s.io/v1 | false | ClusterRoleBinding


Reference: [Using RBAC Authorization](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)
&nbsp;

&nbsp;

# Implement etcd backup and restore

## etcd Backup

- etcd is a key-value store that is used in the cluster to store all cluster data
- It should be backed up on a regular basis

To backup the etcd, different ype of information are needed:

1. **The etcdctl** command, which may not be installed by default
2. The **endpoint** that is used to connect to the etcd
3. **cacert**, which points to the certificate of the etcd service
4. **key**, which points to the etcd service key

> The endpoint is the etcd port.

```bash

ETCDCTL_API=3 etcdctl snapshot save -h

sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --endpoints=http://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt

ETCDCTL_API=3 etcdctl --endpoints=http://127.0.0.1:2379 endpoint status -w table
ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb

```

## etcd Restore

Make sure a snapshotfile is present. It can either be a snapshot file from a previous backup operation, or from a remaining data directory.

```bash
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 snapshot restore snapshotdb
```

If any API servers are running in your cluster, you should not attempt to restore instances of etcd. Instead, follow these steps to restore etcd:

- stop all API server instances
- estore state in all etcd instances
- restart all API server instances

&nbsp;

&nbsp;

# Perform a version upgrade on a Kubernetes cluster using Kubeadm

- yum makecache fast 
- yum list --showduplicates kubeadm --disableexcludes=kubernetes
- yum install -y kubeadm-<version-number> --disableexcludes=kubernetes
- kubectl drain control --ignore-daemonsets
- sudo kubeadm upgrade plan
- sudo kubeadm upgrade apply v<whatever>
- kubectl uncordon control
- sudo yum install -y kubelet-<latestversion> kubectl-<latestversion>
- sudo systemctl daemon-reload
- sudo systemctl restart kubelet

On worker nodes, each by each
- kubectl drain <nodename> --ignore-daemonsets --force --delete-local-data
    if metrics server complain delete it, kubectl delete deployment metrics-server -n kube-system
- worker sudo kubeadm upgrade node
- worker sudo yum install -y kubelet-<latestversion> kubectl-<latestversion> --disableexcludes=kubernetes
- worker sudo system daemon-reload; sudo systemctl restart kubelet
- control kubectl undordon <nodename>
- control kubectl get nodes
&nbsp;

&nbsp;

# Manage a highly#available Kubernetes cluster

Esse aqui fica para depois
