# Services & Networking

## Summary

* [Services &amp; Networking](#services--networking)
   * [Summary](#summary)
   * [1. Understand host networking configuration on the cluster nodes](#1-understand-host-networking-configuration-on-the-cluster-nodes)
      * [Understanding Cluster Networking](#understanding-cluster-networking)
      * [Pod Readiness and Probes](#pod-readiness-and-probes)
         * [Liveness Probes](#liveness-probes)
         * [Readiness probes](#readiness-probes)
         * [startupProbe](#startupprobe)
   * [2. Understand connectivity between Pods](#2-understand-connectivity-between-pods)
      * [Pause Containers and Pod Networking Internals](#pause-containers-and-pod-networking-internals)
      * [Pods connectivity configuration example](#pods-connectivity-configuration-example)
      * [Pod-to-Pod Communication](#pod-to-pod-communication)
      * [Pod-to-Pod configuration example](#pod-to-pod-configuration-example)
      * [NetworkPolicy](#networkpolicy)
         * [Network Policy Identifiers](#network-policy-identifiers)
         * [NetworkPolicy Configuration Example](#networkpolicy-configuration-example)
   * [3. Understand ClusterIP, NodePort, LoadBalancer service types and endpoints](#3-understand-clusterip-nodeport-loadbalancer-service-types-and-endpoints)
      * [Services](#services)
         * [Service configuration example](#service-configuration-example)
      * [ClusterIP](#clusterip)
         * [ClusterIP configuration example](#clusterip-configuration-example)
      * [NodePort](#nodeport)
         * [NodePort configuration example](#nodeport-configuration-example)
      * [LoadBalancer](#loadbalancer)
         * [LoadBalancer configuration example](#loadbalancer-configuration-example)
      * [ExternalName](#externalname)
         * [ExternalName configuration example](#externalname-configuration-example)
   * [4. Know how to use Ingress controllers and Ingress resources](#4-know-how-to-use-ingress-controllers-and-ingress-resources)
      * [Ingress](#ingress)
         * [Ingress Resource configuration example](#ingress-resource-configuration-example)
      * [Ingress Controller](#ingress-controller)
   * [5. Know how to configure and use CoreDNS](#5-know-how-to-configure-and-use-coredns)
      * [Analysing DNS](#analysing-dns)
      * [Troubleshooting DNS](#troubleshooting-dns)
   * [6. Choose an appropriate container network interface plugin](#6-choose-an-appropriate-container-network-interface-plugin)
      * [CNI](#cni)
      * [CNI Configuration](#cni-configuration)
         * [Checking CNI Plugin Configuration](#checking-cni-plugin-configuration)
      * [Managing Network Plugins](#managing-network-plugins)

## 1. Understand host networking configuration on the cluster nodes

### Understanding Cluster Networking

As [Rudi Martinsen](https://rudimartinsen.com/2021/01/10/cka-notes-networking/) concisely stated, "At the heart of the networking model is that all Pods gets assigned an IP address. This means that a Pod can be viewed as a normal VM or Physical host when it comes to networking", thats the greatest Kubernetes network abstraction explanation that I have crossed upon.

Kubernetes relies on the Container Network Interface, CNI, Project to comply with the following requirements:

- All containers must communicate with each other without NAT.

- Nodes can communicate with containers without NAT.

Even with many implementations of Networking Plugins based on CNI, all of them need to follow a few requirements. The most important beying

- Every Pod has a unique IP Address.
- Pods on a node must be able to communicate with pods on all nodes without NAT.
- Agents on a node (kubelet, system daemons etc) must be able to communicate with all pods on that node.
- Pods in the host network of a node can communicate with all pods on all nodes without NAT.

A pod has a unique IP address, which is shared by all containers in the pod. The primary motivation behind giving every pod an IP address is to remove constraints around port numbers.
Kubernetes chose the IP per pod model to be more comfortable for developers to adopt and be easier to run third party workloads. Unfortunately for us, allocating and routing an IP address for every pod adds substantial complexity to a Kubernetes cluster.

The Kubernetes Network is Default-Open by default, Kubernetes will allow any traffic to or from any pod. This passive connectivity means, among other things,that any pod in a cluster can connect to any other pod in that same cluster.That can easily lead to abuse,especially if services do not use authentication or if an attacker obtains credentials.

When explicitly creating a pod, it is possible to specify the IP address. StatefulSets are a built-in workloadtype intended for workloads such as databases, which maintain a pod identity concept and give a new pod the same name and IP address as the pod it replaces.

Every Kubernetes Node runs a component called the kubelet, which manages pods on the node. The networking functionality in the kubelet comes from API interactions with a CNI plugin on the node.

The CNI plugin is what manages pod IP addresses and individual container network provisioning.

Kubernetes does not ship with a default CNI plugin, which means that in a standard installation of Kubernetes, pods cannot use the network.

### Pod Readiness and Probes

kubelet dictates Pod readiness with a user-specified readiness probe.
kubelet can also perform two types of health checks for individual containers in a pod: liveliness probes and readiness probes

#### Liveness Probes

- Pod spec defines the specific liveliness probes to inform the kubelet of the pod's health
- If the probe fails more than the failureThreshold number of times, the kubelet will terminate and restart that container.
- Let the kubelet know when to restart a container.
- When pods use them, they only depend on the container they are testing, with no other dependencies
- Used in specific health check endpoints, which provide minimal validation of criteria.

#### Readiness probes

- Pod Readiness is an additional indication of whether the pod is ready to serve traffic.
- User-specified readiness probes dictates Pod Readiness.
- Determines whether pod address shows up in the endpoints object from an external source.
- Determine when the application inside the container is ready.
- readinessGates specify a list of additional conditions that the kubelet evaluates for Pod readiness.
- conditionType attribute is a condition in the pod's condition list with a matching type.
- When a container's readiness probe fails too many times, the kubelet writes the failure to the pod's status, it doesn't terminate it.

#### startupProbe

- Will inform the kubelet whether the application inside the container is started.
- Takes precedent over the others
- If a startupProbe is defined int he pod spec all other probes are disabled



References:

[Networking](https://kubernetes.io/docs/concepts/cluster-administration/networking/)

## 2. Understand connectivity between Pods

### Pause Containers and Pod Networking Internals

Inside the Pod there is a Pause Container, the pause container is responsible for managing the IP address for the Pod itself and for internal containers in the Pod. Containers in a Pod can communicate with each other using standard inter-process communications (SystemV, POSIX).

Normally the Pod has only one container, but in case there is two or more, we need solutions for stablishing the communication between then, because at the end is the same IP address. Some solutions are:

- IPC(Inter-process communication)
- Shared Volume
- loopback

### Pods connectivity configuration example

To get information about networking in containers we can deploy a Pod and check it:

By deploying two containers on the same Pod, both of the containers will have the same IP Address:

```yaml
kind: Pod
metadata:
  name: net-demo
spec:
  containers:
  - name: busy1
    image: busybox
    command:
    - sleep
    - "3600"
  - name: busy2
    image: busybox
    command:
    - sleep
    - "3600"
```

We can check this by executing the command:

```bash
kubectl exec net-demo -c busy1 -- ip a s

kubectl exec net-demo -c busy2 -- ip a s
```

To check the Pause container created on the Pod we can do this by finding which node is running the Pod:

```bash
kubectl get po -o wide
```

SSH into the node and checking it using Docker(or the Container Runtime Interface equivalent command):

```bash
docker ps | grep pause | grep <pod-name>
```

### Pod-to-Pod Communication

Pods are deployed in one big network, and they are assigned a unique IP Address. Pods are accessible on their specific network. Any Pod can communicate directly with one another and are all in the same network namespace. The CNI provides a framework in which networking modules can be used to establish communication according to different needs. Inter-Pod container communication can be acomplished using IP.

### Pod-to-Pod configuration example

To check this inter-pod communication we can check a Pod IP Address:

```bash
kubectl get pods -o wide
```

We can execute a shell into a container and ping any other Pod IP in the Cluster:

```bash
kubectl exec <pod-name>  -- ping <some-other-pod-ip-address>
```

We can stablish restrictions to connections in the network, for this we use NetworkPolicies.

### NetworkPolicy

NetworkPolicy is a resource type in Kubernetes, which contains allow-based firewall rules. Users can add NetworkPolicy objects to restrict connections to and from pods. The NetworkPolicy resource acts as a configuration for CNI plugins. NetworkPolicy object contains a pod selector, ingress rules, and egress rules. The policy will apply to all pods in the same namespace as the NetworkPolicy that match the selector label.

NetworkPolicies make it possible to implement restrictions on direct traffic between Pods.

- By default, there are no restrictions to network traffic in K8s
- Pods can always communicate, even if they're in other namespaces
- To limit this, Network Policies can be used
- Network Policies need to be supported by the network plugin though
- If in a policy there is no match, traffic will be denied
- If no NetworkPolicy is used, all traffic is allowed

#### Network Policy Identifiers

- In Network Policies, three different identifiers can be used
- Pods: (podSelector) note that a Pod cannot block access to itself
- Namespaces: (namespaceSelector) to grand access to specific namespaces
- IP blocks: (ipBlock) notice that traffic to and from the node where a pod is running is always allowed
- When defining a pod- or namespace-based NetworkPolicy, a selector label is used to specify what traffic is allowed to and from the Pods that match the selector
- Network Policies do not conflict, they are additive.

#### NetworkPolicy Configuration Example

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: access-nginx
spec:
  podSelector:
    matchLabels:
      app: nginx # search for pods with this label
  ingress:
  - from:
    - podSelector:
      matchLabels:
        access: "true" # permit incoming traffic between the pods selected
...
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-np
  labels:
    app: nginx
spec:
  containers:
  - name: nwp-nginx
    image: nginx:1.17
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  labels: 
    app: sleepy
spec:
  containers:
  - name: nwp-busybox
    image: busybox
    command:
    - sleep
    - "3600"
```

A simpler configuration

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: demo
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: demo
  policyTypes:
  - Ingress
  - Egress
  ingress: []NetworkPolicyIngressRule # Not expanded
  egress: []NetworkPolicyEgressRule # Not expanded
```

NetworkPolicy rules act as exceptions, “allow list”, to the default-block caused by selecting pods in a policy.

References:

[Pod Networking](https://kubernetes.io/docs/concepts/workloads/pods/#pod-networking)
[Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

## 3. Understand ClusterIP, NodePort, LoadBalancer service types and endpoints

### Services

A Service is an abstract way to expose an application running on a set of Pods as a network service.

Services in Kubernetes are REST objects, similar to a Pod. It can be used to expose Pods outside the cluster, as long as inside too. Services also provides load balancing between the Pods that defines a Service.

Kubernetes handles services through DNS, so Service names needs to be a valid DNS name. The Service can be reached by his name followed by the namespace he lives within. A Service called **my-service** in the **default** namespace can be reached by the DNS name **my-service.default**.

After defining a Service, it needs to be attached to a Pod to start working, the way that happens is through Label **Selectors**.

A service provides access to Pod **endpoints** by using those **Label Selectors**.

Service types may be especified for exposing an application outside the cluster, those types are listed bellow:

- ClusterIP - Exposes the service on a cluster-internal IP
- NodePort - Exposes the service on each node existing in the cluster as a port.
- LoadBalancer - Expose the service externally using a cloud provider's load balancer.
- ExternalName - Maps a service to a DNS resource.

#### Service configuration example

Define a Service that targets the TCP Port 9376 on any Pod with the label **app=MyApp**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80 # Application Port
      targetPort: 9376 # Pod Port
```

In case you need to abstract other kinds of backends, you can use Service without Selectors:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376 # Object Port
```

Because this Service has no selector, the corresponding Endpoints object is not created automatically. You can manually map the Service to the network address and port where it's running, by adding an **Endpoints** object manually:

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: 192.0.2.42 # ip in the cluster, cannot be the cluster IP for other Kubernetes Services
    ports:
      - port: 9376
```

### ClusterIP

ClusterIP is the default ServiceType, it defines a Service that is only reacheble within the cluster.

#### ClusterIP configuration example

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9377
```

### NodePort

When the NodePort type is defined, the control-plane allocates a port from the cluster, to start serving the service through this port. Each node in the cluster proxy this port into the Service

#### NodePort configuration example

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: MyApp
  ports:
      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.
    - port: 80
      targetPort: 80
      # Optional field
      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)
      nodePort: 30007
```

### LoadBalancer

On cloud providers which support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service. The actual creation of the load balancer happens asynchronously, and information about the provisioned balancer is published in the Service's .status.loadBalancer field.

#### LoadBalancer configuration example

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  clusterIP: 10.0.171.239
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 192.0.2.127
```

Note that every cloud provider has it's own specific configuration for LoadBalancers and support for definying a specific IP address for the LB, may the feature is not supported.

### ExternalName

Services of type ExternalName map a Service to a DNS name, not to a typical selector such as my-service or cassandra. You specify these Services with the spec.externalName parameter.

#### ExternalName configuration example

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
```

References:

[Service Resource](https://kubernetes.io/docs/concepts/services-networking/service/#service-resource)

## 4. Know how to use Ingress controllers and Ingress resources

### Ingress

Ingress resources will help traffic from outside of our cluster reach services within the cluster. Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. This may require a Ingress Controller, that sits on top of Ingress.

The Ingress Controller makes the connection between the **external physical** network and the **internal** **cluster** network.

> For non-HTTP and HTTPS-based traffic, the LoadBalancer or NodePort service objects must be used.

Some considerations about Ingress:

- Advanced traffic routing can be controlled by Ingress rules.
- In the easiest configuration, Ingress forwards all traffic to one service.
- Paths can be used to forward traffic to multiple services.
- Ingress is a loadbalancer on top of a loadbalancer.

#### Ingress Resource configuration example

Traffic comming in with the hostpath /testpath will be routed to the service named test on port 80.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80
```

### Ingress Controller

Ingress resources needs an Ingress controller that is responsible for actually performing what the Ingress resource specifies. An Ingress resource alone has no affect.

It works as a load balancer, and like CNI plugin, Kubernetes have many options for implementing Ingress Controllers, they have specific features accordingly to the provider.

Kubernetes as a project supports and maintains AWS, GCE, and nginx ingress controllers.

References:

[Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)
[Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)

## 5. Know how to configure and use CoreDNS

CoreDNS is the default DNS service in Kubernetes, it is a project and can be replaced with other projects aswell.
Its IP address is exposed by a service *kube-dns* that lives in the kube-system namespace, this service IP address should match the contents of /etc/resolv.conf on the Pod nodes. 

When starting a container, the Kubelet passes DNS to it, using --cluster-dns=<dns-service-ip>.
Also, the kubelet is configured with its local DNS domain, using --cluster-domain=<default-local-domain>

### Analysing DNS

Check Pod DNS resolver, the nameserver should match the IP address of the core-DNS service

```bash
kubectl exec <podname> cat /etc/resolv.conf
```

Verify if the CoreDNS is up

```bash
kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
```

If Pods fail

```bash
kubectl -n kube-system describe pods coredns-...
```

Check logs in CoreDNS Pods

```bash
kubectl -n kube-system logs coredns-...
```

### Troubleshooting DNS

- Verify if the /etc/resolv.conf IP address matches the DNS server IP address.
kubectl get svc -n kube-system -> look for kube-dns ip address

- Disable all firewalling on all nodes iptables -F && iptables -t nat-F && iptables -t mangle -F && iptables -X
- Restart Dockerd: systemctl restart docker
- Remove core-dns Pods: kubectl delete pod -n kube-system -l k8s-app=kube-dns, they are automatically recreated
- Remove your network plugin pod and re-install
- Replace network plugin: Calico is doing better than Weave

References:

[Using CoreDNS for Service Discovery](https://kubernetes.io/docs/tasks/administer-cluster/coredns/)

## 6. Choose an appropriate container network interface plugin

### CNI

CNI is the Container Network Interface, is a Kubernetes proposed framework that helps manage and maintain the pluggable nature of Kubernetes parts, there's also CRI(Container Runtime Interface) and CSI(Container Storage Interface) that is a part of this context.

The CNI plugin is the common plugin used when starting kubelet on a  worker node. It doesn't take care of Pod-to-Pod networking, that is done by the network plugin, the CNI works like a framework so the Network Plugin can be built on top of it, by doing this CNI ensures the pluggable nature of networking, ans makes it easy to select between different network plugins such as linux-bridge, weave and more.

> The use of CNI is common but not standard, if used the --network-plugin=cni argument has been used when starting the kubelet

### CNI Configuration

If CNI is used, the CNI plugin configuration can be found in /etc/cni/net.d, some plugins have the complete network setup in this directory, other plugins have generic settings, and are using additional configuration. Often, the additional configuration for network is implemented by Pods.

#### Checking CNI Plugin Configuration

```bash
$ ls /etc/cni/net.d
10-weave.conflist

cat /etc/cni/net.d/10-weave.conflist
# weave plugin cni config
{
    "cniVersion": "0.3.0",
    "name": "weave",
    "plugins": [
        {
            "name": "weave",
            "type": "weave-net",
            "hairpinMode": true
        },
        {
            "type": "portmap",
            "capabilities": {"portMappings": true},
            "snat": true
        }
    ]
}
```

### Managing Network Plugins

Kubernetes Networking is pluggable, and different plugins are available,configuration often goes through the CNI, from which networking Pods are started by the kubelet on all workernodes.


For checking if the CNI plugin is running in the cluster, we can verify if the pods are up and running within the kube-system namespace. Here we are performing the basic steps to find out if weave is running, and if it is, where it sits on.

```bash
$ kubectl get pods --all-namespaces -o wide | grep weave
kube-system     weave-net-656jj                             2/2     Running   9          18d   10.240.0.22   worker-2       <none>           <none>
kube-system     weave-net-f5l2z                             2/2     Running   9          18d   10.240.0.21   worker-1       <none>           <none>
kube-system     weave-net-fw76c                             2/2     Running   9          18d   10.240.0.20   worker-0       <none>           <none>
kube-system     weave-net-rhs9k                             2/2     Running   12         18d   10.240.0.10   controller-0   <none>           <none>
```

We can see that the weave plugin is running on every node in the cluster on the kube-system namespace, now let's retrieve futher about a weave pod.

```bash
kubectl get pods -n kube-system weave.. -o yaml
...
# some usefull information in the yaml 

kind: DaemonSet
name: weave-net
...
containers:
  - command:
    - /home/weave/launch.sh
    env:
    - name: HOSTNAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
    - name: INIT_CONTAINER
      value: "true"
    image: docker.io/weaveworks/weave-kube:2.8.1
    imagePullPolicy: IfNotPresent
...
volumeMounts:
    - mountPath: /weavedb
      name: weavedb
    - mountPath: /host/var/lib/dbus
      name: dbus
    - mountPath: /host/etc/machine-id
      name: machine-id
      readOnly: true
    - mountPath: /run/xtables.lock
      name: xtables-lock
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-gx6sr
      readOnly: true
# status about the pod behavior  
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2021-07-21T00:26:11Z"
    status: "True"
```

We can investigate a little futher by verifying plugin pod internals

```bash
kubectl -n kube-system exec weave.. ip route show

kubectl -n kube-system exec weave.. ip a s
```

References:

[Network Plugins](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)